## Assigment MOTA-DATA


//  Prompt 1: Diagnosing High CPU / Memory / Disk Usage on Linux VM 
You are a Senior Linux Infrastructure Engineer with deep expertise in VMware vSphere and Linux OS internals.
I am troubleshooting a production Linux VM running on VMware vSphere/ESXi that is experiencing intermittent performance degradation.

 End users report slow application response
 VMware vCenter shows high CPU Ready Time intermittently
 Inside the Linux VM, CPU usage spikes occasionally, memory usage steadily increases, and disk I/O wait is observed during peak hours

Environment details-
- OS: RHEL/CentOS-based Linux
- VM is running on shared ESXi hosts
- VMware Tools is installed and running
- Application is multi-process (Java/Python-based)
- No recent ESXi host failures reported

I want you to:
1. Provide a structured, step-by-step diagnostic approach covering:
   - Linux OS level (CPU, memory, disk, processes)
   - VMware virtualization layer (CPU Ready, ballooning, swapping)
2. List exact Linux commands to run (top, htop, vmstat, iostat, free, sar, pidstat, lsof, etc.) and explain what to look for in their output.
3. Explain how to correlate Linux metrics with vSphere metrics (CPU Ready %, memory ballooning, datastore latency).
4Identify common root causes (CPU overcommitment, memory leaks, noisy neighbors, disk contention, mis-sized VM).
 5.Suggest immediate mitigation steps and long-term fixes (VM resizing, application tuning, reservations/limits, capacity planning).



  //1. Purpose Behind This Prompt
-----The intent is to systematically diagnose performance issues across both the Linux OS 
Istead of treating them in isolation.
Perform systematic troubleshooting .
Correlate Linux OS metrics with VMware virtualization metrics.
Identify whether the issue is caused by:
Application behavior
OS misconfiguration
Resource contention at the hypervisor level

  2. How the AI’s Response Helps
------Provides a production-grade troubleshooting runbook

Helps correlate Linux metrics with vSphere metrics 
Demonstrates understanding of CPU Ready Time, ballooning, swap, I/O wait
Can be directly reused during real incidents or on-call rotation
Provide production-ready automation logic.
Reduce operational risk during:
OS patching
Kernel upgrades
Application deployments
Improve recovery time by enabling one-click rollback.



// Prompt 2: Designing Auto-Recovery Logic for an Unresponsive Linux VM
 You are an expert in Linux automation, reliability engineering, and VMware-based infrastructure operations.
I need to design an automated recovery mechanism for Linux virtual machines hosted on VMware vSphere.
Scenario:
- A Linux VM occasionally becomes unresponsive (SSH hangs, application health checks fail)
- VMware reports the VM as powered on, but OS-level services are stuck
- Manual intervention is currently required (service restart or VM reboot)

Requirements:
1. Design an auto-recovery workflow that:
   - Detects OS unresponsiveness using health checks (SSH, process checks, HTTP checks)
   - Attempts graceful recovery first (restart services)
   - Escalates to VM reboot only if required
2. Propose an architecture using:
   - Shell scripts or Python for health checks
   - Ansible for orchestration
   - Optional VMware APIs or PowerCLI logic for VM-level actions
3. Provide example scripts/playbooks for:
   - Health check logic
   - Decision-making 
   - Logging and alerting
4. Include safeguards to avoid reboot loops and false positives.
5. Explain how this solution would integrate with monitoring tools (Prometheus, Nagios, Zabbix, or vCenter alarms).
How to correlate Linux metrics with vSphere metrics (CPU ready time, ballooning, swapping, datastore latency).
Common root causes (memory leaks, runaway processes, disk saturation, noisy neighbors).
Long-term preventive actions (tuning, alerts, capacity planning)


  // Intent Behind This Prompt
----- The intent is to design a fault-tolerant, self-healing mechanism that reduces manual intervention and improves VM availability.

   //How the AI’s Response Helps
1.CPU contention vs CPU Ready issues
2.Memory leaks vs swapping
3.Application I/O vs datastore latency
4.Offer immediate fixes (restart services, kill processes, rebalance workloads).
5.Suggest preventive strategies like alerts, capacity thresholds, and tuning.
6.Serve as a runbook for junior admins and on-call engineers.




//Technical Strategy Breakdown
Automation Layer: Python + pyVmomi
Safety: Pre-checks, post-checks, rollback logic
Reliability: Logging, exception handling, retention policies
Outcome: Faster recovery, safer patching, reduced downtime

//Performance & Reliability Considerations
Snapshot duration and datastore impact measured during test runs.
Rollback time benchmarked before production rollout.
Alerts integrated for snapshot failures or long-running snapshots.



